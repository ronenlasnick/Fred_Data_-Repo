{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53dc218-8eae-4492-91d3-9a05c06fd227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FRED Pipeline Orchestrator with Frequency Logic\n",
    "# Notebook: 04_fred_orchestrator\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pyspark.sql import Row\n",
    "\n",
    "start_time = time.time()\n",
    "today = datetime.today()\n",
    "month = today.month\n",
    "day = today.day\n",
    "print(f\"[START] Orchestration started at: {today.date()} UTC\")\n",
    "\n",
    "# Track which pipelines actually ran\n",
    "pipelines_ran = []\n",
    "\n",
    "# === Always Run Daily Pipeline ===\n",
    "print(\"\\n[RUNNING] 01_fred_daily_pipeline\")\n",
    "dbutils.notebook.run(\"01_fred_daily_pipeline\", timeout_seconds=1800)\n",
    "pipelines_ran.append(\"daily\")\n",
    "\n",
    "# === Run Monthly on 1st Day of Month ===\n",
    "if day == 1:\n",
    "    print(\"\\n[RUNNING] 02_fred_monthly_pipeline\")\n",
    "    dbutils.notebook.run(\"02_fred_monthly_pipeline\", timeout_seconds=1800)\n",
    "    pipelines_ran.append(\"monthly\")\n",
    "\n",
    "# === Run Quarterly on 1st of Jan, Apr, Jul, Oct ===\n",
    "if day == 1 and month in [1, 4, 7, 10]:\n",
    "    print(\"\\n[RUNNING] 03_fred_quarterly_pipeline\")\n",
    "    dbutils.notebook.run(\"03_fred_quarterly_pipeline\", timeout_seconds=1800)\n",
    "    pipelines_ran.append(\"quarterly\")\n",
    "\n",
    "# === Completion ===\n",
    "end_time = time.time()\n",
    "total_duration = int(end_time - start_time)\n",
    "print(f\"\\n[COMPLETE] Pipelines finished in {total_duration} seconds at {datetime.utcnow()} UTC\")\n",
    "\n",
    "# === Log Metadata ===\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType\n",
    "\n",
    "log_data = [Row(\n",
    "    job_name=\"fred_etl_orchestrator\",\n",
    "    table_name=\"n/a\",\n",
    "    run_date=datetime.utcnow(),\n",
    "    row_count=len(pipelines_ran),\n",
    "    status=\"success\",\n",
    "    duration_sec=total_duration,\n",
    "    frequency=\"daily\"\n",
    ")]\n",
    "\n",
    "log_schema = StructType([\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"run_date\", TimestampType(), True),\n",
    "    StructField(\"row_count\", LongType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"duration_sec\", LongType(), True),\n",
    "    StructField(\"frequency\", StringType(), True)\n",
    "])\n",
    "\n",
    "log_df = spark.createDataFrame(log_data, schema=log_schema)\n",
    "\n",
    "log_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"frequency\") \\\n",
    "    .save(\"/mnt/fred/logs/job_metadata\") \n",
    "\n",
    "# === Register log table for Fabric ===\n",
    "spark.sql(\"DROP TABLE IF EXISTS log_fred_orchestrator\")\n",
    "spark.sql(\"CREATE TABLE log_fred_orchestrator USING DELTA LOCATION '/mnt/fred/logs/job_metadata'\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_fred_orchestrator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
